p_testing_data <- training_data[-partitioned_data, ]
plot(p_training_data$classe, col="green", main="Bar Plot of levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
rfModel <- randomForest(classe ~ ., data = p_training_data, importance = TRUE, ntrees = 10)
predit_training <- predict(rfModel, p_training_data)
confusionMatrix(predit_training, p_training_data$classe)
predit_testing <- predict(rfModel, p_testing_data)
confusionMatrix(predit_testing, p_testing_data$classe)
ptest <- predict(random_model, testing_data)
ptest
random_model <- randomForest(classe ~ ., data = p_training_data, importance = TRUE, ntrees = 10)
predit_training <- predict(random_model, p_training_data)
confusionMatrix(predit_training, p_training_data$classe)
predit_testing <- predict(random_model, p_data)
confusionMatrix(predit_testing, p_data$classe)
p_data <- training_data[-partitioned_data, ]
predit_testing <- predict(random_model, p_data)
confusionMatrix(predit_testing, p_data$classe)
ptest <- predict(random_model, testing_data)
ptest
pml_write_files(ptest)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(ptest)
setwd("C:/Users/Admin/Downloads/final/final/en_US")
setwd("C:/Users/Admin/Downloads/final/en_US")
fileName="en_US.twitter.txt"
con=file(fileName,open="r")
lineTwitter=readLines(con)
longTwitter=length(line)
close(con)
biostatsTwitter<-grep("biostats",lineTwitter)
lineTwitter[biostatsTwitter]
grep -x "A computer once beat me at chess, but it was no match for me at kickboxing"
biostatsTwitter<-grep("The guy in front of me just bought a pound of bacon, a bouquet, and a case of",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("The guy in front of me just bought a pound of baconf",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("bacon",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("bacon, a bouquet, and a case of",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("a bouquet",lineTwitter)
lineTwitter[biostatsTwitter]
setwd("C:/Users/Admin/Downloads/final/de_DE")
fileName="de_DE.twitter.txt"
con=file(fileName,open="r")
lineTwitter=readLines(con)
longTwitter=length(line)
close(con)
biostatsTwitter<-grep("a bouquet",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("bouquet",lineTwitter)
lineTwitter[biostatsTwitter]
setwd("C:/Users/Admin/Downloads/final/en_US")
fileName="en_US.twitter.txt"
con=file(fileName,open="r")
lineTwitter=readLines(con)
longTwitter=length(line)
close(con)
biostatsTwitter<-grep("a case",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("the guy",lineTwitter)
lineTwitter[biostatsTwitter]
biostatsTwitter<-grep("me just bought",lineTwitter)
lineTwitter[biostatsTwitter]
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='tangtc', token='C4D4451D9F07A7AAB1DFFC2CFF3B2F89', secret='jBQQaXfNoxfmte2gA7ENrGSy0MEH884Gq2hO6Hmb')
setwd("C:/Users/Admin/Downloads/final/en_US")
fileName="en_US.blogs.txt"
con=file(fileName,open="r")
lineBlogs=readLines(con)
longBlogs=length(line)
close(con)
fileName="en_US.news.txt"
con=file(fileName,open="r")
lineNews=readLines(con)
longNews=length(line)
close(con)
fileName="en_US.twitter.txt"
con=file(fileName,open="r")
lineTwitter=readLines(con)
longTwitter=length(line)
close(con)
txt1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
txt2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
txt3 <-"Hey sunshine, can you follow me and make me the"
txt4 <- "Very early observations on the Bills game: Offense still struggling but the"
txt5 <-"Go on a romantic date at the"
txt6 <-"Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
txt7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
txt8 <-"After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
txt9 <- "Be grateful for the good times and keep the faith during the"
txt10 <- "If this isn't the cutest thing you've ever seen, then you must be"
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
library(tm)
library(SnowballC); library(RWeka); library(rJava); library(RWekajars)
install.packages('SnowballC')
install.packages('RWeka')
install.packages('RWeka')
install.packages('tm')
txt1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
txt2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
txt3 <-"Hey sunshine, can you follow me and make me the"
txt4 <- "Very early observations on the Bills game: Offense still struggling but the"
txt5 <-"Go on a romantic date at the"
txt6 <-"Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
txt7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
txt8 <-"After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
txt9 <- "Be grateful for the good times and keep the faith during the"
txt10 <- "If this isn't the cutest thing you've ever seen, then you must be"
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
library(tm)
txt1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
txt2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
txt3 <-"Hey sunshine, can you follow me and make me the"
txt4 <- "Very early observations on the Bills game: Offense still struggling but the"
txt5 <-"Go on a romantic date at the"
txt6 <-"Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
txt7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
txt8 <-"After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
txt9 <- "Be grateful for the good times and keep the faith during the"
txt10 <- "If this isn't the cutest thing you've ever seen, then you must be"
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
library(stringi)
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
install.packages('rJava')
install.packages('RWekajars')
library(stringi)
library(SnowballC); library(RWeka); library(rJava); library(RWekajars)
library(tm)
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
library(wordcloud)
install.packages('wordcloud')
library(wordcloud)
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
trainingData<- paste(sample(enNews,5000),sample(enTwitter,1000),sample(enBlogs,3000),sep=" ")
rm(enNews,enTwitter,enBlogs)
enNews <- readLines("en_US/en_US.news.txt",skipNul = TRUE,encoding = 'UTF-8')
enTwitter <- readLines("en_US/en_US.twitter.txt",skipNul = TRUE,encoding = 'UTF-8')
enBlogs <- readLines("en_US/en_US.blogs.txt",skipNul = TRUE,encoding = 'UTF-8')
trainingData<- paste(sample(lineNews,5000),sample(lineTwitter,1000),sample(lineBlogs,3000),sep=" ")
rm(lineNews,lineTwitter,lineBlogs)
ascllen <- stri_enc_toascii(trainingData)
trainingData <- stri_replace_all_regex(ascllen,'\032','')
rm(ascllen)
trcorpus <- Corpus(VectorSource(trainingData))
trcorpus <- Corpus(VectorSource(trainingData))
object.size(trcorpus)
trcorpus <- tm_map(trcorpus,content_transformer(tolower))
trcorpus <- tm_map(trcorpus,removePunctuation)
trcorpus <- tm_map(trcorpus,removeNumbers)
trcorpus <- tm_map(trcorpus,removeWords,stopwords("english"))
for(j in seq(trcorpus))
{
trcorpus[[j]] <- gsub("/", " ", trcorpus[[j]])
trcorpus[[j]] <- gsub("@", " ", trcorpus[[j]])
trcorpus[[j]] <- gsub("([A-Za=z][A-Za-z][A-Za-z]+)", " ", trcorpus[[j]])
}
trcorpus <- tm_map(trcorpus, function(x) iconv(enc2utf8(x), sub = "byte"))
library(SnowballC)
trcorpus <- tm_map(trcorpus,stemDocument,language = ("english"))
detach(package:SnowballC)
trcorpus <- tm_map(trcorpus,stripWhitespace)
trcorpus <- tm_map(trcorpus,PlainTextDocument)
object.size(trcorpus)
library(RWeka)
library(tm)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
trTDMBi <- TermDocumentMatrix(trcorpus, control = list(tokenize = BigramTokenizer))
trSparse <- removeSparseTerms(trTDMBi,0.999)
#Decrease this  number to get more efficient algorithm . Currently for intermediate report experimenting with a bigger set
dim(trTDMBi)
trWordfrequency <- rowSums(as.matrix(trSparse))
ord <- order(trWordfrequency)
txt1 <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
txt2 <- "You're the reason why I smile everyday. Can you follow me please? It would mean the"
txt3 <-"Hey sunshine, can you follow me and make me the"
txt4 <- "Very early observations on the Bills game: Offense still struggling but the"
txt5 <-"Go on a romantic date at the"
txt6 <-"Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
txt7 <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
txt8 <-"After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
txt9 <- "Be grateful for the good times and keep the faith during the"
txt10 <- "If this isn't the cutest thing you've ever seen, then you must be"
testing <- function(txt){
txt <- removeNumbers(tolower(txt))
txt <- removePunctuation(txt)
txt <- removeWords(txt, stopwords("english"))
txt <- stripWhitespace(txt)
txt <- stemDocument(txt)
txt<-unlist(strsplit(txt," "))
for(i in 1:length(txt)){
print(txt[i])
corrtxt<- findAssocs(trSparse, txt[i],corlimit=0.2)
print(corrtxt)
}
}
testing(txt1)
testing(txt1)
quiz22 <- lowerenb[grepl('struggling',lowerenb)]
enb.vec <- VectorSource(quiz22)
enb <- Corpus(enb.vec)
enblog.tdm <- TermDocumentMatrix(enb)
BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}
bdm <- TermDocumentMatrix(enb, control = list(tokenize = BigramTokenizer))
case <- bdm$dimnames$Terms[grepl('mean the',bdm$dimnames$Terms)]
m = as.matrix(bdm[case,])
v = sort(rowSums(m), decreasing = TRUE)
install.packages(c('ggplot2', 'shiny'))
library(shiny)
library(ggplot2)
function(input, output) {
dataset <- reactive({
diamonds[sample(nrow(diamonds), input$sampleSize),]
})
output$plot <- renderPlot({
p <- ggplot(dataset(), aes_string(x=input$x, y=input$y)) + geom_point()
if (input$color != 'None')
p <- p + aes_string(color=input$color)
facets <- paste(input$facet_row, '~', input$facet_col)
if (facets != '. ~ .')
p <- p + facet_grid(facets)
if (input$jitter)
p <- p + geom_jitter()
if (input$smooth)
p <- p + geom_smooth()
print(p)
}, height=700)
}
library(shiny)
library(ggplot2)
dataset <- diamonds
fluidPage(
titlePanel("Diamonds Explorer"),
sidebarPanel(
sliderInput('sampleSize', 'Sample Size', min=1, max=nrow(dataset),
value=min(1000, nrow(dataset)), step=500, round=0),
selectInput('x', 'X', names(dataset)),
selectInput('y', 'Y', names(dataset), names(dataset)[[2]]),
selectInput('color', 'Color', c('None', names(dataset))),
checkboxInput('jitter', 'Jitter'),
checkboxInput('smooth', 'Smooth'),
selectInput('facet_row', 'Facet Row', c(None='.', names(dataset))),
selectInput('facet_col', 'Facet Column', c(None='.', names(dataset)))
),
mainPanel(
plotOutput('plot')
)
)
library(shiny)
runApp()
getwd()
setw("C:\Users\Admin\Desktop\Data Science\Module 9 - Developing Data Products\Project")
setw(@"C:\Users\Admin\Desktop\Data Science\Module 9 - Developing Data Products\Project")
setw("C:/Users/Admin/Desktop/Data Science/Module 9 - Developing Data Products/Project")
setwd("C:/Users/Admin/Desktop/Data Science/Module 9 - Developing Data Products/Project")
getwd()
library(shiny)
runApp()
rsconnect::getAccountInfo()
library(rsconnect)
rsconnect::getAccountInfo()
shinyapps::setAccountInfo(name='tangtc', token='C4D4451D9F07A7AAB1DFFC2CFF3B2F89', secret='jBQQaXfNoxfmte2gA7ENrGSy0MEH884Gq2hO6Hmb')
deployApp()
library(shiny)
library(rsconnect)
runApp()
require(data.table)
# library(sqldf)
library(dplyr)
library(DT)
library(rCharts)
# Read data
data <- fread("./data/postscndryunivsrvy2013dirinfo.csv")
getwd()
dim(data)
data <- fread("./data/postscndryunivsrvy2013dirinfo.csv")
data <- fread("./data/postscndryunivsrvy2013dirinfo.csv")
data <- fread("./data/postscndryunivsrvy2013dirinfo.csv")
dim(data)
head(data)
sum(is.na(data))
setnames(data, "_id", "setId")
data <- fread("./data/sparql.csv")
dim(data)
head(data)
data <- fread("./data/2011-Q1-cabi-trip-history-data.csv")
dim(data)
head(data)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
bikes <- sort(unique(dataset$Bike))
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
install.packages("formatR")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
library("lubridate")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shinyapps::setAccountInfo(name='tangtc', token='C4D4451D9F07A7AAB1DFFC2CFF3B2F89', secret='jBQQaXfNoxfmte2gA7ENrGSy0MEH884Gq2hO6Hmb')
library(shiny)
library(rsconnect)
shinyapps::setAccountInfo(name='tangtc', token='C4D4451D9F07A7AAB1DFFC2CFF3B2F89', secret='jBQQaXfNoxfmte2gA7ENrGSy0MEH884Gq2hO6Hmb')
deployApp()
